
#Kaggle examples: 

###•	Titanic data exploration, seaborn visualization, and machine learning.
*The titanic dataset is used to predict the probability of survival based on gender, passenger class, age, family size, and other variables as seen in the image below.*  I used Seaborn for visualization and Sklearn for logistic regression and random forest models.  I cleaned the dataset and used logistic regression and random forest for models; I then used cross validation to choose the best model.

![Alt text](https://github.com/heavenstobetsy/Kaggle/blob/master/images/Titanic.PNG "Titanic Variables")

###•	San Francisco Crime data exploration and visualization.
*Exploring crime data in San Fransisco. A density map of larcency/theft in San Fransisco, broken out by year, as seen in the image below.*  Uses Seaborn for visualization.

![Alt text](https://github.com/heavenstobetsy/Kaggle/blob/master/images/density.plot.2014.2015.PNG "San Fransico Crime")


###•	Santander's customer satisfaction data: data exploration, cleaning, and random forest model.
*The Santander dataset is used to identify dissatisfied customers early on, especially before the customer leaves.  This dataset has hundreds of features used to predict whether a customer is satisfied/dissatisfied with their banking experience*  I used a random forest model, and used cross validation and checked the fit using AUC (area under curve).



###•	Tweets during the first GOP debate in August: data exploration and visualization
*I created word clouds of Tweets mentioning Trump and Jeb Bush, in an outline of the US.*  Uses WordCloud package for visualization.

![Alt text](https://github.com/heavenstobetsy/Kaggle/blob/master/images/PoliticsTrumpTweets.PNG "Trump Tweets")



###•	Predicting shelter animal outcomes: adoption, death, euthanasia, return to owner, and transfer: adaptive bbooster classifier.
*This is a multi-class classification problem. Data exploration, visualization, and classification.*  Currently a work in progress.  I cleaned the data and used sklearn: first I used an adaboost classifier and used log loss as an evaluation metric.  Next up is a random forest model.

![Alt text](https://github.com/heavenstobetsy/Kaggle/blob/master/images/Shelter.PNG "Shelter Adoptions")
